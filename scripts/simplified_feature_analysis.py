#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰¹å¾é‡è¦æ€§åˆ†æ
å¿«é€Ÿåˆ†æç‰¹å¾é‡è¦æ€§ï¼Œä¸åŒ…å«å¯è§†åŒ–éƒ¨åˆ†
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import f_regression, mutual_info_regression
from sklearn.model_selection import train_test_split
import lightgbm as lgb
import json
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class SimplifiedFeatureAnalyzer:
    def __init__(self):
        self.feature_importance = {}
        self.feature_rankings = {}
        self.results = {}
        
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        print("ğŸ“Š åŠ è½½æ•°æ®...")
        data = pd.read_csv('data/processed_features.csv')
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {data.shape}")
        return data
    
    def prepare_features(self, data, target_col='future_return_1'):
        """å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡"""
        print("ğŸ”§ å‡†å¤‡ç‰¹å¾æ•°æ®...")
        
        # åˆ›å»ºç›®æ ‡å˜é‡
        if target_col not in data.columns:
            print(f"âš ï¸ ç›®æ ‡åˆ— {target_col} ä¸å­˜åœ¨ï¼Œåˆ›å»ºæœªæ¥æ”¶ç›Šç‡...")
            data[target_col] = data['close'].pct_change().shift(-1)
        
        # é€‰æ‹©ç‰¹å¾åˆ—
        feature_cols = [col for col in data.columns if col not in 
                       ['date', 'symbol', 'close', 'open', 'high', 'low', 'volume', target_col]]
        
        print(f"ğŸ“‹ å¯ç”¨ç‰¹å¾æ•°é‡: {len(feature_cols)}")
        
        # å‡†å¤‡æ•°æ®
        X = data[feature_cols].fillna(0)
        y = data[target_col].fillna(0)
        
        # ç§»é™¤æ— æ•ˆæ•°æ®
        valid_idx = ~(np.isinf(X).any(axis=1) | np.isinf(y) | np.isnan(y))
        X = X[valid_idx]
        y = y[valid_idx]
        feature_names = X.columns.tolist()
        
        print(f"ğŸ“Š æœ€ç»ˆæ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
        return X, y, feature_names
    
    def analyze_lightgbm_importance(self, X, y, feature_names):
        """LightGBMç‰¹å¾é‡è¦æ€§åˆ†æ"""
        print("ğŸš€ LightGBMç‰¹å¾é‡è¦æ€§åˆ†æ...")
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        model = lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)
        model.fit(X_train, y_train)
        
        importance = model.feature_importances_
        feature_importance = dict(zip(feature_names, importance))
        
        return feature_importance
    
    def analyze_random_forest_importance(self, X, y, feature_names):
        """éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§åˆ†æ"""
        print("ğŸŒ² éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§åˆ†æ...")
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
        model.fit(X_train, y_train)
        
        importance = model.feature_importances_
        feature_importance = dict(zip(feature_names, importance))
        
        return feature_importance
    
    def analyze_statistical_importance(self, X, y, feature_names):
        """ç»Ÿè®¡å­¦ç‰¹å¾é‡è¦æ€§åˆ†æ"""
        print("ğŸ“ˆ Fç»Ÿè®¡é‡ç‰¹å¾é‡è¦æ€§åˆ†æ...")
        
        f_scores, _ = f_regression(X, y)
        feature_importance = dict(zip(feature_names, f_scores))
        
        return feature_importance
    
    def create_comprehensive_ranking(self, importance_dict):
        """åˆ›å»ºç»¼åˆæ’å"""
        print("ğŸ† åˆ›å»ºç»¼åˆç‰¹å¾æ’å...")
        
        # è·å–æ‰€æœ‰ç‰¹å¾
        all_features = set()
        for method_importance in importance_dict.values():
            all_features.update(method_importance.keys())
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¹³å‡æ’å
        feature_avg_rank = {}
        
        for feature in all_features:
            ranks = []
            for method, importance in importance_dict.items():
                if feature in importance:
                    # æŒ‰é‡è¦æ€§æ’åºï¼Œè·å–æ’å
                    sorted_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)
                    rank = next(i for i, (f, _) in enumerate(sorted_features, 1) if f == feature)
                    ranks.append(rank)
            
            if ranks:
                feature_avg_rank[feature] = np.mean(ranks)
        
        # æŒ‰å¹³å‡æ’åæ’åº
        sorted_features = sorted(feature_avg_rank.items(), key=lambda x: x[1])
        
        return sorted_features, feature_avg_rank
    
    def analyze_feature_categories(self, feature_rankings):
        """åˆ†æç‰¹å¾ç±»åˆ«æ€§èƒ½"""
        print("ğŸ“Š åˆ†æç‰¹å¾ç±»åˆ«æ€§èƒ½...")
        
        # å®šä¹‰ç‰¹å¾ç±»åˆ«
        categories = {
            'volatility_features': ['volatility', 'std', 'atr', 'srvi'],
            'trend_features': ['sma', 'ema', 'macd', 'rsi', 'adx'],
            'price_features': ['close', 'open', 'high', 'low', 'prev', 'price'],
            'volume_features': ['volume', 'vwap', 'obv'],
            'momentum_features': ['momentum', 'roc', 'williams'],
            'other_features': []
        }
        
        # åˆ†ç±»ç‰¹å¾
        feature_category_map = {}
        for feature, rank in feature_rankings.items():
            categorized = False
            for category, keywords in categories.items():
                if category == 'other_features':
                    continue
                if any(keyword in feature.lower() for keyword in keywords):
                    feature_category_map[feature] = category
                    categorized = True
                    break
            if not categorized:
                feature_category_map[feature] = 'other_features'
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¹³å‡æ’å
        category_performance = {}
        for category in categories.keys():
            category_features = [f for f, c in feature_category_map.items() if c == category]
            if category_features:
                avg_rank = np.mean([feature_rankings[f] for f in category_features])
                category_performance[category] = {
                    'avg_rank': avg_rank,
                    'feature_count': len(category_features),
                    'features': category_features[:5]  # æ˜¾ç¤ºå‰5ä¸ªç‰¹å¾
                }
        
        return category_performance
    
    def test_feature_combinations(self, X, y, top_features, feature_names):
        """æµ‹è¯•ä¸åŒç‰¹å¾ç»„åˆçš„æ€§èƒ½"""
        print("ğŸ§ª æµ‹è¯•ç‰¹å¾ç»„åˆæ€§èƒ½...")
        
        from sklearn.metrics import mean_squared_error
        from sklearn.linear_model import Ridge
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        results = {}
        
        # æµ‹è¯•ä¸åŒæ•°é‡çš„é¡¶çº§ç‰¹å¾
        for n_features in [5, 10, 15, 20, 30]:
            if n_features > len(top_features):
                continue
                
            selected_features = [f[0] for f in top_features[:n_features]]
            feature_indices = [feature_names.index(f) for f in selected_features if f in feature_names]
            
            if len(feature_indices) == 0:
                continue
            
            X_train_selected = X_train.iloc[:, feature_indices]
            X_test_selected = X_test.iloc[:, feature_indices]
            
            # è®­ç»ƒç®€å•æ¨¡å‹
            model = Ridge(alpha=1.0, random_state=42)
            model.fit(X_train_selected, y_train)
            
            y_pred = model.predict(X_test_selected)
            mse = mean_squared_error(y_test, y_pred)
            
            results[f'top_{n_features}'] = {
                'mse': mse,
                'features': selected_features,
                'feature_count': len(selected_features)
            }
        
        return results
    
    def generate_recommendations(self, top_features, category_performance, combination_results):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºæœ€ä½³ç‰¹å¾ç±»åˆ«çš„å»ºè®®
        best_category = min(category_performance.items(), key=lambda x: x[1]['avg_rank'])
        recommendations.append(f"é‡ç‚¹å…³æ³¨{best_category[0]}ç‰¹å¾ï¼Œå¹³å‡æ’åæœ€ä½³: {best_category[1]['avg_rank']:.2f}")
        
        # åŸºäºé¡¶çº§ç‰¹å¾çš„å»ºè®®
        top_5_features = [f[0] for f in top_features[:5]]
        recommendations.append(f"ä¼˜å…ˆä½¿ç”¨Top 5ç‰¹å¾: {', '.join(top_5_features)}")
        
        # åŸºäºç»„åˆæµ‹è¯•çš„å»ºè®®
        if combination_results:
            best_combo = min(combination_results.items(), key=lambda x: x[1]['mse'])
            recommendations.append(f"æœ€ä½³ç‰¹å¾ç»„åˆ: {best_combo[0]} (MSE: {best_combo[1]['mse']:.6f})")
        
        return recommendations
    
    def save_results(self, results):
        """ä¿å­˜ç»“æœ"""
        print("ğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")
        
        os.makedirs('output', exist_ok=True)
        
        with open('output/simplified_feature_analysis_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print("âœ… ç»“æœå·²ä¿å­˜åˆ° output/simplified_feature_analysis_results.json")
    
    def run_analysis(self, target_col='future_return_1'):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸ¯ å¼€å§‹ç®€åŒ–ç‰¹å¾é‡è¦æ€§åˆ†æ")
        print("=" * 50)
        
        try:
            # åŠ è½½æ•°æ®
            data = self.load_data()
            
            # å‡†å¤‡ç‰¹å¾
            X, y, feature_names = self.prepare_features(data, target_col)
            
            # å¤šç§æ–¹æ³•åˆ†æç‰¹å¾é‡è¦æ€§
            importance_methods = {
                'lightgbm': self.analyze_lightgbm_importance(X, y, feature_names),
                'random_forest': self.analyze_random_forest_importance(X, y, feature_names),
                'f_statistic': self.analyze_statistical_importance(X, y, feature_names)
            }
            
            # åˆ›å»ºç»¼åˆæ’å
            top_features, feature_rankings = self.create_comprehensive_ranking(importance_methods)
            
            # åˆ†æç‰¹å¾ç±»åˆ«
            category_performance = self.analyze_feature_categories(feature_rankings)
            
            # æµ‹è¯•ç‰¹å¾ç»„åˆ
            combination_results = self.test_feature_combinations(X, y, top_features, feature_names)
            
            # ç”Ÿæˆå»ºè®®
            recommendations = self.generate_recommendations(top_features, category_performance, combination_results)
            
            # æ•´ç†ç»“æœ
            results = {
                'timestamp': datetime.now().isoformat(),
                'target_column': target_col,
                'data_shape': {'samples': X.shape[0], 'features': X.shape[1]},
                'top_features': top_features[:20],  # å‰20ä¸ªç‰¹å¾
                'category_performance': category_performance,
                'combination_results': combination_results,
                'recommendations': recommendations,
                'individual_importance': importance_methods
            }
            
            # ä¿å­˜ç»“æœ
            self.save_results(results)
            
            # æ‰“å°æ€»ç»“
            print("\nğŸ“Š åˆ†æå®Œæˆ! ä¸»è¦å‘ç°:")
            print(f"ğŸ† Top 10ç‰¹å¾: {[f[0] for f in top_features[:10]]}")
            if category_performance:
                best_cat = min(category_performance.items(), key=lambda x: x[1]['avg_rank'])
                print(f"ğŸ¯ æœ€ä½³ç‰¹å¾ç±»åˆ«: {best_cat[0]} (å¹³å‡æ’å: {best_cat[1]['avg_rank']:.2f})")
            if combination_results:
                best_combo = min(combination_results.items(), key=lambda x: x[1]['mse'])
                print(f"âš¡ æœ€ä½³ç‰¹å¾ç»„åˆ: {best_combo[0]} (MSE: {best_combo[1]['mse']:.6f})")
            
            return results
            
        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return None

def main():
    analyzer = SimplifiedFeatureAnalyzer()
    results = analyzer.run_analysis()
    
    if results:
        print("\nâœ… ç‰¹å¾é‡è¦æ€§åˆ†æå®Œæˆ!")
    else:
        print("\nâŒ åˆ†æå¤±è´¥!")

if __name__ == "__main__":
    main()